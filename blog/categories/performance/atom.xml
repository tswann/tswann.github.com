<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Performance | X1B Plan]]></title>
  <link href="http://tswann.github.io/blog/categories/performance/atom.xml" rel="self"/>
  <link href="http://tswann.github.io/"/>
  <updated>2014-07-04T23:01:23+01:00</updated>
  <id>http://tswann.github.io/</id>
  <author>
    <name><![CDATA[Tom Swann]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Dynamics CRM Data Migration]]></title>
    <link href="http://tswann.github.io/blog/2014/01/25/dynamics-crm-data-migration/"/>
    <updated>2014-01-25T20:07:00+00:00</updated>
    <id>http://tswann.github.io/blog/2014/01/25/dynamics-crm-data-migration</id>
    <content type="html"><![CDATA[<p>For some bizarre reason I&rsquo;m ridiculously early for a flight. Hence some blogging time!</p>

<p>This is a much belated follow up to my previous post <a href="http://esc-plan.com/blog/2013/10/23/dynamics-crm-performance-tuning/">Dynamics CRM Performance Tuning</a>.</p>

<p>I&rsquo;m going to take a look at some of the issues involved in planning and executing a large migration exercise into Dynamics CRM 2011 from one or more legacy systems.</p>

<p>I&rsquo;ll cover aspects of planning, designing and developing a solution, and look at how to address some of the more pressing non-functional issues like performance and scalability when dealing with large volumes of data.</p>

<p>Dear Reader, let&rsquo;s do this thing.</p>

<!-- more -->


<h2>The Migration Scenario</h2>

<p>Generally speaking, legacy LOB systems are often messy affairs when it comes to data quality.</p>

<p>Over a large number of years of development, support and unnumbered godless hacks, there <em>will</em> be everything and anything in there. You never know quite what murky, red-eyed creations are lurking in the corners of these old data stores!</p>

<p><img src="http://i.imgur.com/lV3vFPg.jpg?1" title="Here Be Ninjas" alt="Here Be Ninjas" /></p>

<p>When migrating (or indeed, just integrating) with such systems, we need to be on our toes with regards to issues of data integrity, data loss, data cleansing, data creation&hellip; basically a long list of non-trivial stuff.</p>

<p>A common scenario for undertaking a data migration exercise is one where the customers current system (or systems) are no longer fit for purpose.</p>

<p>There could be many reasons why this might be the case; here are just a few:</p>

<ul>
<li>Mis-alignment with business goals (e.g. the business may wish to adopt a more lean, customer-centric philosophy. Old systems often tend to be product-centric. This is a common scenario when migrating to Dynamics CRM.)</li>
<li>Older technology does not offer easy extensibility.</li>
<li>Costly licensing/support.</li>
<li>Language requires specialised skills making it difficult to come by individuals capable of maintaining it.</li>
</ul>


<p>Every Dynamics project I&rsquo;ve worked on has had some element of migration, from ongoing integration with legacy systems to their wholesale replacement.</p>

<p>Many of the issues we&rsquo;ll look at are pretty fundamental to any data migration exercise, though obviously Dynamics has it&rsquo;s own particular considerations and technical quirks.</p>

<p>In this post I&rsquo;ll try to cover both. The scenario is of a customer wishing to migrate a large set of their data from multiple legacy systems into a brand new solution based around Dynamics CRM 2011. We&rsquo;re dealing with CRM On-Premise here, but a lot of this will apply equally well to the On-line (cloud) version.</p>

<h2>Project Approach</h2>

<p>Let&rsquo;s say that our new system is in the process of being designed and built and that our task to migrate records from the old world to the new will be progressing in parallel with the main stream of development.</p>

<p>Starting to think about data migration early in the lifecycle of the project is critical. In the beginning we will have a limited understanding of the legacy systems, and on the other side of the coin the customer will be trying to understand their requirements for the new system and how their existing data can be made to fit.</p>

<p>It can be hard to get a sense early on of how much effort will be involved in data migration, particularly if the requirements of the new system are liable to shift (<em>hint</em>: they will).</p>

<p>An iterative process which allows for regular feedback in both directions will save you a world of pain and make that process of reaching mutual understanding with the customer go that much smoother.</p>

<h2>Defining The Process</h2>

<p><strong>ETL</strong> (Extract Transform Load) is a label applied to processes which essentially involve the transfer of data from point A to point B via some set of transformation rules. This is the conceptual frame for how our migration exercise will work.</p>

<p>In our case point A encompasses several legacy systems, let&rsquo;s call them Legacy X and Legacy Y. Point B is the Dynamics CRM 2011 database.</p>

<p>We have a technical limitation in that our interaction with Dynamics is via it&rsquo;s web services &ndash; direct access to the database is unsupported. So that will be our channel for getting data in. Obviously a web service is much slower on the face of it than a straight DB insert, so this will impact on the design of our load process and how we scale for performance.</p>

<p>Here&rsquo;s a high-level overview of how our ETL process will look:</p>

<p><img src="https://dl.dropboxusercontent.com/u/47685018/Blog/2014/01-26/Migration_ETL.png" title="Example Migration Process" alt="&quot;Example Migration Process&quot;" /></p>

<h2>1. Source System Extracts</h2>

<p>The intial step in the process is to extract the data from the customers existing source systems.</p>

<p>How this is accomplished will depend on the nature of the source systems and who is responsible for producing the extract &ndash; you, the customer or some third party. Typically you&rsquo;ll agree on the subset of source tables which will come across and the format of the data within each file.</p>

<p>In my example we receive two sets of CSV files from each system, one file for each source table.</p>

<p>At this point I want to bring those files into SQL Server so that I can easily analyse them as my ETL process progresses. Each system is assigned a set of tables which each correspond to a file. Each table is prefixed to identify the source. In this case that&rsquo;s <strong>X_ </strong> and <strong>Y_ </strong>.</p>

<p>To insert the data into SQL Server I use good old <a href="http://technet.microsoft.com/en-us/library/ms162802.aspx">BCP</a>. Each input file has an associated FMT (format) file which tells BCP how to map the source columns to the destination table.</p>

<p>I can also redirect any failures out to a log file; so this is the first step in the process where I can validate the quality of the data and get a feel for any work that I will need to do around field seperators, quoted strings etc.</p>

<h2>2. Transformation</h2>

<p>The second step is the where the bulk of the effort is going to be &ndash; devising a set of transformation rules to convert sets of old records in the X and Y import tables and inserting them into the staging tables (STG) ready for load.</p>

<p>The staging tables are effectively a mirror of the target entities in Dynamics CRM, though limited to only the subset of fields which we actually need to populate.</p>

<p>Transformation rules can be developed as SQL Server stored procedures which select from the necessary source tables, apply some mapping rules and then insert into the relevant stg_ table in the new format.</p>

<p>For an example at the level of a single field, a character based status code &lsquo;A&rsquo; which denotes an active record in the old system might well translate into an numeric OptionSet code in CRM (e.g. 948100000)</p>

<h2>3. Load</h2>

<p>My load step is accomplished by a custom C# utility which sucks the records out of the staging tables using Entity Framework and inserts them into Dynamics CRM via the SOAP web service endpoint.</p>

<p>If you build a level of convention into the format of the staging tables, then this final step should be relatively trivial.</p>

<p>I name each stg_* table after the corresponding record in Dynamics CRM into which it will be loaded. For example, stg_Account, stg_Contact and so on. Each staging table column is given the same name as the corresponding column in Dynamics CRM.</p>

<p>Building linkages between different entity types can be achieved in the staging tables by generating the GUIDs for each record up front during step 2. You need to be careful of the order in which you load your entities into Dynamics CRM to avoid missing reference errors. For example if loading in Contacts which have a parent Account, you will need to have loaded the Accounts in first.</p>

<p>A good idea is to make the list of entities which your application will load configurable via App.config. This will allow you to easily test subsets of the entities independently of the others.</p>

<p>Given that I have already defined my staging schema as part of the exercise to transform the source data, I make use of <a href="http://msdn.microsoft.com/en-gb/data/jj206878.aspx">Entity Framework Database First</a> to automagically generate a set of classes in C# which I can use to extract the information from the staging tables.</p>

<p>Because of the naming conventions I have adopted in my data layer, I can use <a href="https://github.com/AutoMapper/AutoMapper">AutoMapper</a> to easily map the generated POCOS to the early-bound Dynamics CRM classes (also generated) with minimal code.</p>

<p><div><script src='https://gist.github.com/8649357.js'></script>
<noscript><pre><code>using AutoMapper;
using Microsoft.Xrm.Sdk;

public class AutoMapperProfile : Profile
{
  protected override void Configure()
  {
    this.RecognizePrefixes(&quot;new_&quot;);
    
    this.CreateMap&lt;stg_Contact, Contact)
      .ForMember(dest =&gt; dest.ParentCustomerId, o =&gt; o.MapFrom(src =&gt; new EntityReference(Account.EntityLogicalName, src.ParentCustomerId)));
  }
}</code></pre></noscript></div>
</p>

<p>The gist above shows how you can add mappings to an AutoMapper profile. Basic types can be mapped without supplying an explicit mapping since our source and destination field names match.</p>

<p>Note as well how we can supply a recognised prefix &ndash; this means that our stg<em> tables can leave out the &lsquo;new</em>&rsquo; (or whatever letters your Organisation is using) that Dynamics CRM prepends onto all custom fields.</p>

<p>OptionSets and EntityReferences will need either a Converter class to handle them implicitly or they can be mapped explicitly as in the example above.</p>

<h2>Non-Functional Requirements</h2>

<p>The performance achieved when loading into Dynamics CRM via the SOAP web service can be slow if you are dealing with a single Application server instance. Gains can be made by scaling up the DB and App servers, however the biggest gains will come from installing multiple instances of the App server in a load balanced configuration and running your Load process in parallel against each one. This might be your only option to achieve practical timings when dealing with big sets of records (say 50 million + and if you only have a weekend in which to complete your migration).</p>

<p>Some other performance points to consider:</p>

<ul>
<li>Turning off plugins and workflows. This can be done programatically via the API, so include pre and post migration steps in your Loader code to disable and re-enable these.</li>
<li>Disable any nightly backup/maintenance jobs. This is kind of an obvious one, but make sure there&rsquo;s an explicit task for someone to ensure these are turned off. The last thing you want is some overnight job killing your app if the run time for your load is long.</li>
<li>If using Entity Framework to load your data, remember that this is just a bulk read-only task. Therefore make sure you turn context tracking off, otherwise your garbage collector will scream into a pillow and jump off the roof. Below is an example method to get all records to type T with context tracking turned off. Pretty straightforward.</li>
</ul>


<p><div><script src='https://gist.github.com/8674031.js'></script>
<noscript><pre><code>public IQueryable&lt;T&gt; GetAll&lt;T&gt;() where T : class
{
    return _stagingContext.Set&lt;T&gt;().AsNoTracking().AsQueryable();
}</code></pre></noscript></div>
</p>

<h2>Alternatives</h2>

<p>Taking a step back for a moment, you should note that I have developed individual steps for each part of the ETL process over which I have very fine grained control. You could also go down the <a href="http://technet.microsoft.com/en-us/library/ms141026.aspx">SSIS</a> route to manage the whole process for you end-to-end.</p>

<p>There&rsquo;s a lot to be said for this option if you have SSIS skills and want to keep the whole flow within a single tool. Personally (and &ndash; <strong>DISCLAIMER</strong> &ndash; not being an SSIS expert) I found it a bit restrictive. There is an option for connection to CRM in this <a href="http://www.kingswaysoft.com/products/ssis-integration-toolkit-for-microsoft-dynamics-crm">tool from Kingswaysoft</a>. This provides a graphical way to build mappings between the staging tables and CRM.</p>

<p>Again, under the hood this is using the same Dynamics CRM web services API that any custom code would use. Personally I would rather write a minimal amount of code using something like AutoMapper than go clicking lots of checkboxes in a GUI, but <strong>that&rsquo;s just me</strong>.</p>

<h2>Finally</h2>

<p>That&rsquo;s my Dynamics CRM data migration approach at a fairly high level. I&rsquo;d be genuinely interested to learn how others have approached this, so please leave a comment if you&rsquo;ve been through the mill on this yourself.</p>

<p>Back to staring at the flight board!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Dynamics CRM 2011 Performance Tuning: Part One]]></title>
    <link href="http://tswann.github.io/blog/2013/10/23/dynamics-crm-performance-tuning/"/>
    <updated>2013-10-23T12:53:00+01:00</updated>
    <id>http://tswann.github.io/blog/2013/10/23/dynamics-crm-performance-tuning</id>
    <content type="html"><![CDATA[<p>In recent months I&rsquo;ve been dealing with some big sets of data as part of a Dynamics CRM 2011 project.</p>

<p>This post is the first in a series that amounts to a bit of a brain dump on the topic of performance &ndash; some lessons learned and some useful techniques for tuning.</p>

<p>I hope to get a somewhat more regular stream of content on here in the coming months. I&rsquo;ve got an eye on Dynamics 2013 and some other non-CRM related gubbins that I&rsquo;ve been pondering as well.</p>

<!-- more -->


<h2>Tuning Views</h2>

<p><blockquote><p>Just a note on CRM version &ndash; this tuning was carried out against an on-premise installation running CRM 2011 Rollup 11. Subsequent rollups have included updates specifically to address application performance, so some of this could potentially be of limited relevance in more current versions. Some of it will I&rsquo;m sure still be generally applicable, and probably deep in the realm of common sense.</p></blockquote></p>

<p>Good view design in Dynamics CRM is about really filtering out the noise and presenting users with the information which is most relevant to them.</p>

<p>For big data sets (multiple millions of records) it&rsquo;s rare that you would want a view that didn&rsquo;t do some serious filtering on this, whether that be restricting the view to records belonging to the user, a given date range or a particular option set range.</p>

<p>Views in Dynamics CRM are all paged out of the box; the default page size is 50 with an option to increase this to a maximum of 250 which gets applied for all entities across the board.</p>

<h2>Profiling SQL Server</h2>

<p>If you fire up SQL Server Profiler and take a peek under the hood what you find is that the queries being executed against the database apply a TOP using this configurable value.</p>

<p>Now by and large you would think this to be absolutely fine. However it&rsquo;s possible to get into difficulties and I&rsquo;ve seen a variation of around 20 seconds in the speed of a view loading which was purely down to the sort order of the columns!</p>

<p>Below is the offending order by clause, which was generated for a custom entity:</p>

<p><div><script src='https://gist.github.com/0cc4eea312d1dac7829d.js'></script>
<noscript><pre><code>order by
 case &quot;new_claim0&quot;.new_Type 
 when 948100000 then @new_Type0 
 when 948100001 then @new_Type1 
 when 948100002 then @new_Type2 
 when 948100003 then @new_Type3 
 when 948100004 then @new_Type 
 else null end asc</code></pre></noscript></div>
</p>

<p>@new_TypeX in the above statement equals the textual description of each value in the OptionSet.</p>

<p>The CASE statement in the order by clause here is applied to all the records in that large data set, before the TOP is applied.</p>

<p>If you use a default system generated view which by default filters to show only Active records, and that count of Active records is in the order of millions, then this is going to cause you a major performance problem.</p>

<p>You might not even realise it either. A sort order of some kind is mandatory on a CRM view, and by default the view designer applies this to the first column you add. So if that first column just so happens to be an OptionSet then you could well have this one lurking in your system, just waiting for the record counts to get adequately large to start seriously killing the view response times.</p>

<p>Changing your sort order to a non-option set field &ndash; say a date &ndash; and applying an index to that field in the database will get you right back down to sub 1 second response times. So it&rsquo;s easily enough rectified, but can be easily missed &ndash; remember this is a system designed to be configured by it&rsquo;s business users &ndash; not the sort of people who will be considering the performance of under-the-hood generated SQL.</p>

<h2>Further Reading</h2>

<p>The following article on MSDN contains some more detail on indexing for Dynamics: <a href="http://msdn.microsoft.com/en-us/library/jj126126.aspx"></a></p>

<p>See also the following: <a href="http://technet.microsoft.com/en-us/library/hh413200.aspx#BKMK_Optimize_Data_Tier">Optimizing the Data-Tier</a></p>

<h2>Next Up</h2>

<p>Designing indexes to support your front-end view design and including sensible filters are key to maintaining reasonable performance in a large Dynamics CRM installation.</p>

<p>In a subsequent post I&rsquo;ll look at a few issues to consider when using the Dynamics API. I&rsquo;ll also look at some strategies for data migration.</p>

<p><strong><em>EDIT</em></strong> Follow-up to this post now available: <a href="http://esc-plan.com/blog/2014/01/25/dynamics-crm-data-migration/">Dynamics CRM Data Migration</a></p>
]]></content>
  </entry>
  
</feed>
